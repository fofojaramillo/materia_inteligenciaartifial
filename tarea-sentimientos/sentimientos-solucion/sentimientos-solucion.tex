\documentclass[14pt,a4paper]{report}
\usepackage[utf8]{inputenc}
%Idioma
\usepackage[spanish]{babel}
\spanishdecimal{.}
%Para expresiones matematicas
\usepackage{titlesec}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{physics}
%Imagenes
\usepackage{graphicx}
\usepackage{color}
%Para dibujar
\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{babel,patterns,snakes}
\usetikzlibrary{shapes.callouts}
\usetikzlibrary{calc,patterns,angles,quotes}
\usetikzlibrary{shadings}
%Cajas con colores
\usepackage{tcolorbox}
\tcbuselibrary{theorems}
%Ejemplo de lo anterior
%\begin{equation}
% a x^2 + bx + c = 0 \rightarrow
%\tcboxmath[colback=magenta!25!white,colframe=magenta, title=Solución]
%{x = \frac{-b\pm\sqrt{b^2-4ac}}{2a}}  
%\end{equation}
%
%Estilo fancy
\usepackage{fancyhdr}
%Interlineado y margenes y poco mas
\usepackage{setspace}
\usepackage{parskip}
\usepackage{multicol}
\usepackage[left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
%Empieza documento
\begin{document}
%Cajas comentadas 
\newcommand{\commentedbox}[2]{%
  \mbox{
    \begin{tabular}[t]{@{}c@{}}
    $\boxed{\displaystyle#1}$\\
    #2
    \end{tabular}%
  }%
}
%Definimos el estilo de las paginas
\pagestyle{fancy}
\lhead{\itshape Semestre 2023-1}
\rhead{Tarea 0}
\Large{\textit{Inteligencia Artificial} - Sentimientos}\\
\normalsize
Rodolfo Armando Jaramillo Ruiz\\
25 de Febrero de 2023\\
\subsection*{1. Construyendo la intuición}
\quad Se tiene el siguiente conjunto de mini-reseñas:
\begin{enumerate}
	\item $(-1)$ not good
	\item $(-1)$ pretty bad
	\item $(+1)$ good plot
	\item $(+1)$ pretty scenery
\end{enumerate}
A cada reseña se le asocia un vector de características $\phi(x)$ donde se asocia a cada palabra la cantidad de veces que aparece en la reseña. Por ejemplo:\\
\begin{equation*}
	\phi(x)=\{\text{not}:1,\text{good}:1\}
\end{equation*} 
Se usa la definición de pérdida de articulación
\begin{equation*}
	\text{Loss}_{\text{hinge}}=\text{max}\{0,1-\bm{w}\phi(x)y\}
\end{equation*}
Todo donde $x$ es el texto de la reseña, $y$ es la etiqueta correcta y $\bm{w}$ es el vector de pesos.\\
\textbf{a.} Supongamos que corremos el descenso de gradiente estocastico una vez por cada una de las cuatro muestras en el orden dado, actualizando los pesos de acuerdo a:
\begin{equation*}
	\mathbf{w}\leftarrow\mathbf{w}-\eta\nabla_{w}\text{Loss}_{\text{hinge}}
\end{equation*}
Despues de las actualizaciones, ¿cuales son los pesos de las seis palabras ("pretty",
"good", "bad", "plot", "not", "scenery")que aparecen en las reseñas de arriba?\\
\begin{itemize}
	\item $\eta=0.1$
	\item $\mathbf{w}$ inicial: $\mathbf{w}=[0,0,0,0,0,0]$
	\item $\nabla_{w}\text{Loss}_{\text{hinge}}$ es cero cuando el margen es exactamente 1
\end{itemize}
Se empieza calculando el $\nabla_{w}\text{Loss}_{\text{hinge}}$ que sería la siguiente expresión:
\begin{equation*}
	\nabla_{w}\text{Loss}_{\text{hinge}}(x,y,\mathbf{w})=\left\{ \begin{array}{lcc}
             -\phi(x)y &   si  & 1-\mathbf{w}\cdot\phi > 0 \\
             \\ 0 &  \text{en otro caso} \\
             \end{array}
   \right.
\end{equation*}
Para la primera reseña tenemos lo siguiente:\\
\begin{equation*}
	\phi(x)=\{\text{not}:1,\text{good}:1\}=[0,1,0,0,1,0]
\end{equation*}
Por lo que
\begin{equation*}
	-\phi(x)y=-[0,1,0,0,1,0](-1)=[0,1,0,0,1,0]
\end{equation*}
Entonces $\mathbf{w}$ queda
\begin{equation*}
	\mathbf{w}=[0,0,0,0,0,0]-0.1[0,1,0,0,1,0]
\end{equation*}
\begin{equation*}
	\mathbf{w}=[0,-0.1,0,0,-0.1,0]
\end{equation*}
Para la segunda reseña tenemos lo siguiente:\\
\begin{equation*}
	\phi(x)=\{\text{pretty}:1,\text{bad}:1\}=[1,0,1,0,0,0]
\end{equation*}
Por lo que
\begin{equation*}
	-\phi(x)y=-[1,0,1,0,0,0](-1)=[1,0,1,0,0,0]
\end{equation*}
Entonces $\mathbf{w}$ queda
\begin{equation*}
	\mathbf{w}=[0,-0.1,0,0,-0.1,0]-0.1[1,0,1,0,0,0]
\end{equation*}
\begin{equation*}
	\mathbf{w}=[-0.1,-0.1,-0.1,0,-0.1,0]
\end{equation*}
Para la tercera reseña tenemos lo siguiente:\\
\begin{equation*}
	\phi(x)=\{\text{good}:1,\text{plot}:1\}=[0,1,0,1,0,0]
\end{equation*}
Por lo que
\begin{equation*}
	-\phi(x)y=-[0,1,0,1,0,0](1)=[0,-1,0,-1,0,0]
\end{equation*}
Entonces $\mathbf{w}$ queda
\begin{equation*}
	\mathbf{w}=[0,-0.1,0,0,-0.1,0]-0.1[0,-1,0,-1,0,0]
\end{equation*}
\begin{equation*}
	\mathbf{w}=[-0.1,0,-0.1,0.1,-0.1,0]
\end{equation*}
Para la cuarta reseña tenemos lo siguiente:\\
\begin{equation*}
	\phi(x)=\{\text{pretty}:1,\text{scenary}:1\}=[1,0,0,0,0,1]
\end{equation*}
Por lo que
\begin{equation*}
	-\phi(x)y=-[1,0,0,0,0,1](1)=[-1,0,0,0,0,-1]
\end{equation*}
Entonces $\mathbf{w}$ queda
\begin{equation*}
	\mathbf{w}=[-0.1,0,-0.1,0.1,-0.1,0]-0.1[-1,0,0,0,0,-1]
\end{equation*}
Tenemos finalmente:
\begin{equation*}
	\mathbf{w}=[0,0,-0.1,0.1,-0.1,-0.1]
\end{equation*}
\textbf{b.} Dado el siguiente conjunto de reseñas
\begin{enumerate}
	\item $(-1)$ bad
	\item $(1)$ good
	\item $(+1)$ not bad
	\item $(-1)$ not good
\end{enumerate}
Muestra que no hay clasificador lineal que utilice características de palabras que tenga
cero error sobre este conjunto de datos. Recuerda que esta es una pregunta sobre clasificadores, no sobre algoritmos de optimización; tu demostración debe ser correcta para cualquier clasificador lineal, sin importar en cómo se aprenden los pesos.\\
El vector de pesos para este conjunto de reseñas es:\\
\begin{equation*}
	\mathbf{w}=[w_{bad},w_{good},w_{not}]
\end{equation*}
Los vectores de caracteristicas correspondientes son
\begin{equation*}
\begin{array}{cc}
\phi(x)=\{\text{bad}:1\} & \phi(x)=\{\text{not}:1,\text{bad}:1\} \\
\phi(x)=\{\text{good}:1\} &  \phi(x)=\{\text{not}:1,\text{bad}:1\} \\
\end{array}
\end{equation*}
De aquí se entiende el siguiente sistema de ecuaciones:
\begin{equation*}
\begin{array}{c}
w_{bad}=-1\\
w_{good}=1\\
w_{bad}+w_{not}=1\\
w_{good}+w_{not}=-1
\end{array}
\end{equation*}
Que no tiene solución, es inconsistente, por lo tanto, no existe un clasificador lineal que tenga cero error.\\
Se puede añadir la siguiente característica\\
\begin{equation*}
	cos(\mathbf{w}\cdot\phi(x))
\end{equation*}
Para que el sistema tenga solución, y en consecuencia haya un clasificador lineal que tenga cero error.
\subsection*{2. Prediciendo calificadores de películas}
Supongamos que estamos interesados en predecir una calificación numérica para reseñas de películas. Vamos a usar un predictor no-lineal que toma una reseña de películas x y regresa $\sigma(\mathbf{w}\cdot\phi(x))$, donde $\sigma(z)=(1+e^{z})^{-1}$ es la función logística que aplasta un numero real al rango (0, 1). Para este problema, supón que la calificación de películas y es una variable con valor real en el
rango [0, 1].\\
\textbf{a.} La pérdida cuadratica tiene la siguiente forma:
\begin{equation*}
	\text{Loss}(x,y,\mathbf{w})=(\sigma(\mathbf{w}\cdot\phi(x))-y)^2
\end{equation*}
\textbf{b.} El gradiente de la perdida cuadratica se calcula como sigue:
\begin{equation*}
	\text{Loss}(x,y,\mathbf{w})=((1+e^{-\mathbf{w}\cdot\phi(x)})^{-1}-y)^2
\end{equation*}
\begin{equation*}
	\nabla_{w}\text{Loss}(x,y,\mathbf{w})=2((1+e^{-\mathbf{w}\cdot\phi(x)})^{-1}-y)\cdot(-1)(1+e^{-\mathbf{w}\cdot\phi(x)})^{-2}\cdot(e^{-\mathbf{w}\cdot\phi(x)})\phi(x)
\end{equation*}
\begin{equation*}
	\nabla_{w}\text{Loss}(x,y,\mathbf{w})=2((1+e^{-\mathbf{w}\cdot\phi(x)})^{-1}-y)\cdot(1+e^{-\mathbf{w}\cdot\phi(x)})^{-2}\cdot(e^{-\mathbf{w}\cdot\phi(x)})\phi(x)
\end{equation*}
\begin{equation*}
	\nabla_{w}\text{Loss}(x,y,\mathbf{w})=
	2(\sigma(\mathbf{w}\cdot\phi(x))-y)\cdot(\sigma(\mathbf{w}\cdot\phi(x)))^{2}\cdot(\sigma(\mathbf{w}\cdot\phi(x))^{-1}-1)\phi(x)
\end{equation*}
Considerando $p=\sigma(\mathbf{w}\cdot\phi(x))$:
\begin{equation*}
	\nabla_{w}\text{Loss}(x,y,\mathbf{w})=
	2(p-y)\cdot(p)^{2}\cdot(p^{-1}-1)\phi(x)
\end{equation*}
\textbf{c.} Si consideramos $\mathbf{w}$ pequeño el primer termino con $\sigma(\mathbf{w}\cdot\phi(x))$ se acerca solamente a $1/2$, en consecuencia el segundo termino se acerta a $1/4$, y el ultimo termino se acerca a la unidad, por lo tanto hacer $\mathbf{w}$ cada vez más pequeño converge a un valor diferente de cero. Ahora, con $\mathbf{w}$ muy grande, el $\sigma(\mathbf{w}\cdot\phi(x))$ contenido en el primer termino se acerca a la unidad, pero como $\mathbf{w}$ es finito nunca llega a cero, solo se hace cada vez más pequeño, al igual que en el tercer termino. En sintesis, con $\mathbf{w}$ grande el gradiente se acerca arbitrariamente a cero, pero nunca puede ser completamente cero.






















\end{document}